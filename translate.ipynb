{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f663182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rohit\\anaconda3\\lib\\site-packages (4.6.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\rohit\\anaconda3\\lib\\site-packages (0.1.95)\n",
      "Requirement already satisfied: datasets in c:\\users\\rohit\\anaconda3\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (0.0.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (4.49.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: dill in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (0.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece datasets\n",
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d45e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\anaconda3\\envs\\st\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from IPython.html import widgets\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from transformers import AdamW, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "701d0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = 'google/mt5-small'\n",
    "model_path = 'C:/Users/rohit/Documents/Coding/en-mr/models/mt5_translation.pt'\n",
    "max_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc58acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777b68f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model description: https://huggingface.co/google/mt5-base\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_repo)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace8065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1042,    282,    286,    669,   1494,    898,    390,  37194,    285,\n",
      "            288,  30865,    309,    274, 116024,  11994,    271,      1]],\n",
      "       device='cuda:0')\n",
      "tensor([[     0, 250099,      1]], device='cuda:0')\n",
      "<pad> <extra_id_0></s>\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    '<mr> This will be translated to Japanese! (hopefully)',\n",
    "    return_tensors='pt').cuda()\n",
    "print(token_ids)\n",
    "\n",
    "model_out = model.generate(token_ids)\n",
    "print(model_out)\n",
    "\n",
    "output_text = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(model_out[0]))\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8507622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[1042,  282,  286,  669, 1494,  339, 1627,  259,  262, 2978,  259,  272,\n",
      "         1982, 1315,  260,    1]])\n",
      "Tokens: ['▁<', 'm', 'r', '>', '▁This', '▁is', '▁just', '▁', 'a', '▁test', '▁', 'n', 'bu', 'ig', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "example_input_str = '<mr> This is just a test nbuig.'\n",
    "# example_input_str = 'これは普通のテスト'\n",
    "input_ids = tokenizer.encode(example_input_str, return_tensors='pt')\n",
    "print('Input IDs:', input_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "print('Tokens:', tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83e72d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sorted(tokenizer.vocab.items(), key=lambda x: x[1])[1000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c26f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://huggingface.co/datasets/alt\n",
    "#dataset = load_dataset('alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9340f7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_dataset = dataset['train']\\ntest_dataset = dataset['test']\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3186683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_TOKEN_MAPPING = {\n",
    "    'en': '<en>',\n",
    "    'mr': '<mr>'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c195aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250102, 512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': list(LANG_TOKEN_MAPPING.values())}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd011a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<mr> This is just a test nbuig.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a929ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"en-mr/train.tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "#train_df[\"prefix\"] = \"\"\n",
    "train_df=train_df[::2]\n",
    "train_df=train_df.rename(columns = {'input_text':'mr','target_text':'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c98acb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[250101,   1494,    339,   1627,    259,    262,   2978,    259,    272,\n",
      "           1982,   1315,    260,      1,      0,      0,      0,      0,      0,\n",
      "              0,      0]])\n",
      "['<mr>', '▁This', '▁is', '▁just', '▁', 'a', '▁test', '▁', 'n', 'bu', 'ig', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(\n",
    "    example_input_str, return_tensors='pt', padding='max_length',\n",
    "    truncation=True, max_length=max_seq_len)\n",
    "print(token_ids)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195cea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input_str(text, target_lang, tokenizer, seq_len,\n",
    "                     lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  target_lang_token = lang_token_map[target_lang]\n",
    "\n",
    "  # Tokenize and add special tokens\n",
    "  input_ids = tokenizer.encode(\n",
    "      text = target_lang_token + text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "\n",
    "  return input_ids[0]\n",
    "  \n",
    "def encode_target_str(text, tokenizer, seq_len,\n",
    "                      lang_token_map=LANG_TOKEN_MAPPING):\n",
    "  token_ids = tokenizer.encode(\n",
    "      text = text,\n",
    "      return_tensors = 'pt',\n",
    "      padding = 'max_length',\n",
    "      truncation = True,\n",
    "      max_length = seq_len)\n",
    "  \n",
    "  return token_ids[0]\n",
    "\n",
    "def format_translation_data(translations, lang_token_map,\n",
    "                            tokenizer, seq_len=128):\n",
    "  # Choose a random 2 languages for in i/o\n",
    "  langs = list(lang_token_map.keys())\n",
    "  input_lang, target_lang = np.random.choice(langs, size=2, replace=False)\n",
    "\n",
    "  # Get the translations for the batch\n",
    "  input_text = translations[input_lang]\n",
    "  target_text = translations[target_lang]\n",
    "\n",
    "  if input_text is None or target_text is None:\n",
    "    return None\n",
    "\n",
    "  input_token_ids = encode_input_str(\n",
    "      input_text, target_lang, tokenizer, seq_len, lang_token_map)\n",
    "  \n",
    "  target_token_ids = encode_target_str(\n",
    "      target_text, tokenizer, seq_len, lang_token_map)\n",
    "\n",
    "  return input_token_ids, target_token_ids\n",
    "\n",
    "def transform_batch(batch, lang_token_map, tokenizer):\n",
    "  inputs = []\n",
    "  targets = []\n",
    "  for i in range(0,len(batch)*2,2):\n",
    "    #print(batch.loc[i])\n",
    "    formatted_data = format_translation_data(\n",
    "        batch.loc[i], lang_token_map, tokenizer, max_seq_len)\n",
    "    \n",
    "    if formatted_data is None:\n",
    "      continue\n",
    "    \n",
    "    input_ids, target_ids = formatted_data\n",
    "    inputs.append(input_ids.unsqueeze(0))\n",
    "    targets.append(target_ids.unsqueeze(0))\n",
    "    \n",
    "  batch_input_ids = torch.cat(inputs).cuda()\n",
    "  batch_target_ids = torch.cat(targets).cuda()\n",
    "\n",
    "  return batch_input_ids, batch_target_ids\n",
    "\n",
    "def get_data_generator(dataset, lang_token_map, tokenizer, batch_size=32):\n",
    "  for i in range(0, len(dataset), batch_size):\n",
    "    raw_batch = dataset[i:i+batch_size]\n",
    "    yield transform_batch(raw_batch, lang_token_map, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39159ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8, 20])\n",
      "Output shape: torch.Size([8, 20])\n"
     ]
    }
   ],
   "source": [
    "# Testing `data_transform`\n",
    "in_ids, out_ids = format_translation_data(\n",
    "    train_df.loc[0], LANG_TOKEN_MAPPING, tokenizer)\n",
    "\n",
    "#print(' '.join(tokenizer.convert_ids_to_tokens(in_ids)))\n",
    "#print(' '.join(tokenizer.convert_ids_to_tokens(out_ids)))\n",
    "\n",
    "# Testing data generator\n",
    "data_gen = get_data_generator(train_df, LANG_TOKEN_MAPPING, tokenizer, 8)\n",
    "data_batch = next(data_gen)\n",
    "print('Input shape:', data_batch[0].shape)\n",
    "print('Output shape:', data_batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d142a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8fc6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 8\n",
    "batch_size = 4\n",
    "print_freq = 50\n",
    "checkpoint_freq = 1000\n",
    "lr = 5e-4\n",
    "n_batches = int(np.ceil(len(train_df) / batch_size))\n",
    "total_steps = n_epochs * n_batches\n",
    "n_warmup_steps = int(total_steps * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c4bdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, n_warmup_steps, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5156fefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "266a1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, gdataset, max_iters=8):\n",
    "  test_generator = get_data_generator(gdataset, LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "  eval_losses = []\n",
    "  for i, (input_batch, label_batch) in enumerate(test_generator):\n",
    "    if i >= max_iters:\n",
    "      break\n",
    "\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "    eval_losses.append(model_out.loss.item())\n",
    "\n",
    "  return np.mean(eval_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f54195bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-ca2e7375f0fc>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  in tqdm_notebook(enumerate(data_generator), total=n_batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b76673341a458797ef7cd693927097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=739997.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 6.00 GiB total capacity; 4.07 GiB already allocated; 0 bytes free; 4.32 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-ca2e7375f0fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\st\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\st\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\st\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                     \u001b[1;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg_sq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"exp_avg_sq\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 490.00 MiB (GPU 0; 6.00 GiB total capacity; 4.07 GiB already allocated; 0 bytes free; 4.32 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(1):\n",
    "  # Randomize data order\n",
    "  data_generator = get_data_generator(train_df[0:10000], LANG_TOKEN_MAPPING,\n",
    "                                      tokenizer, batch_size)\n",
    "                \n",
    "  for batch_idx, (input_batch, label_batch) \\\n",
    "      in tqdm_notebook(enumerate(data_generator), total=n_batches):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    model_out = model.forward(\n",
    "        input_ids = input_batch,\n",
    "        labels = label_batch)\n",
    "\n",
    "    # Calculate loss and update weights\n",
    "    loss = model_out.loss\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training update info\n",
    "    if (batch_idx + 1) % print_freq == 0:\n",
    "      avg_loss = np.mean(losses[-print_freq:])\n",
    "      print('Epoch: {} | Step: {} | Avg. loss: {:.3f} | lr: {}'.format(\n",
    "          epoch_idx+1, batch_idx+1, avg_loss, scheduler.get_last_lr()[0]))\n",
    "      \n",
    "    if (batch_idx + 1) % checkpoint_freq == 0:\n",
    "      test_loss = eval_model(model, test_dataset)\n",
    "      print('Saving model with test loss of {:.3f}'.format(test_loss))\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8215c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e177305",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df[::2]\n",
    "train_df.rename(columns = {'input_text':'mr','target_text':'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e26ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.rename(columns = {'input_text':'mr','target_text':'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c989775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581100fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in train_df[:1000]:\n",
    "    print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f775d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st",
   "language": "python",
   "name": "st"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
